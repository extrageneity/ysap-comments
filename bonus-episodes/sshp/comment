I wrote a pure bash tool to do this for a previous employer. Yours seems better and more performant than mine was, but mine did one thing that it doesn't look like yours supports. Via command line switches, you could specify a file that would be passed to all the ssh commands as stdin; if the first character of this "filename" was a pipe, the remaining characters of that optarg would be evaluated as a shell pipeline, once per SSH. With this I could do things like:
        query-service-mesh-cmd | pcmd -i helper_script.sh bash -l -s -- arg1 arg2
... and dynamically query my service mesh (at that time, Hashicorp Consul) for a list of healthy hosts, and my tool would ssh into those systems in bounded parallel, spawning bash out of the $PATH for the target user on each host, and feeding a script to run into each of those bash instances, and even supplying arguments to that script.

Your version handles output much more versatilely and elegantly than mind did, I just made an output directory and wrote one file per target host with the combined stdout/stderr stream from each host plus its exit code, and then tracked exit codes to show whether all commands executed or whether there were outliers. 

Mine could also run in a local mode where instead of launching ssh, it would operate more like xargs, but setting up the shell environment where the command executed to know which host was the target host for each parallelized run.

The reason I implemented my own in bash instead of using one of the other options (even something generic like GNU Parallel or xargs -P) was that my employer had a remote access pattern where each person in the ops team all had to log in to production from a shared bastion host, with separate sudo-capable "landing" users on each production machine. My script wrapped some shell functions a co-worker had written which calculated the name of the user, fetched user-specific auth credentials which would let the employee target their own landing user on the remote host, and then sudo to the target user on the remote host. Trying to get xargs to interoperate with shell functions was fairly nightmarish, and spawning an independent bash shell to load those bash functions once per host did not scale well.

These days that same kind of parallelized technique can be extremely valuable alongside non-SSH based architectures, such as public clouds where you can use cloud provider specific commands to shell into their VMs/containers, or Kubernetes where you can run 'kubectl exec' to do the same kind of thing.

I love this as a video for your channel because even though you don't show what your source code is doing, you demonstrate something that bash isn't very good at: output multiplexing. Your code relies on the epoll() family of system calls, which is one of a few kernel interfaces to which bash doesn't implement a direct interface.

If you decide to do a Rust-based bash builtin for the channel, a version of the read builtin which lets it poll multiple streams, and sets a variable like $REPLY_FD to tell the caller which handle the input line came from, would be sick. With that builtin, I think it might be possible to make a complete bash port of your sshp utility, using coprocesses to set up the children.
